\documentclass[10pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{pdfpages} 
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\pagestyle{fancy}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}
\usepackage{tikz}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{url}
\usepackage{dsfont}
\usepackage{amssymb,amsmath}
\usepackage{xspace}


\usepackage{xcolor}

\lhead{
\textbf{University of Waterloo}
}
\rhead{\textbf{Winter 2024}
}
\chead{\textbf{
SYDE675
 }}

\newcommand{\Id}{\mathbb{I}}
\newcommand{\NN}{\mathds{N}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\RR}{\mathds{R}}
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\magenta}[1]{{\color{magenta}#1}}


\newcommand{\ea}{{et al.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\iid}{{i.i.d.}\xspace}
\newcommand{\cf}{{cf.}\xspace}
\newcommand{\wrt}{{w.r.t.}\xspace}
\newcommand{\aka}{{a.k.a.}\xspace}
\newcommand{\etc}{{etc.}\xspace}

\newcommand{\ans}[1]{{\color{orange}\textsf{Ans}: #1}}


\lfoot{}
\cfoot{\textbf{Ali Ayub (a9ayub@uwaterloo.ca) \textcopyright 2024}}

%================================
%================================

\setlength{\parskip}{1cm}
\setlength{\parindent}{1cm}
\tikzstyle{titregris} =
[draw=gray,fill=white, shading = exersicetitle, %
text=gray, rectangle, rounded corners, right,minimum height=.3cm]
\pgfdeclarehorizontalshading{exersicebackground}{100bp}
{color(0bp)=(green!40); color(100bp)=(black!5)}
\pgfdeclarehorizontalshading{exersicetitle}{100bp}
{color(0bp)=(red!40);color(100bp)=(black!5)}
\newcounter{exercise}
\renewcommand*\theexercise{exercice \textbf{Exercice}~n\arabic{exercise}}
\makeatletter
\def\mdf@@exercisepoints{}%new mdframed key:
\define@key{mdf}{exercisepoints}{%
\def\mdf@@exercisepoints{#1}
}

\mdfdefinestyle{exercisestyle}{%
outerlinewidth=1em,outerlinecolor=white,%
leftmargin=-1em,rightmargin=-1em,%
middlelinewidth=0.5pt,roundcorner=3pt,linecolor=black,
apptotikzsetting={\tikzset{mdfbackground/.append style ={%
shading = exersicebackground}}},
innertopmargin=0.1\baselineskip,
skipabove={\dimexpr0.1\baselineskip+0\topskip\relax},
skipbelow={-0.1em},
needspace=0.5\baselineskip,
frametitlefont=\sffamily\bfseries,
settings={\global\stepcounter{exercise}},
singleextra={%
\node[titregris,xshift=0.5cm] at (P-|O) %
{~\mdf@frametitlefont{\theexercise}~};
\ifdefempty{\mdf@@exercisepoints}%
{}%
{\node[titregris,left,xshift=-1cm] at (P)%
{~\mdf@frametitlefont{\mdf@@exercisepoints points}~};}%
},
firstextra={%
\node[titregris,xshift=1cm] at (P-|O) %
{~\mdf@frametitlefont{\theexercise}~};
\ifdefempty{\mdf@@exercisepoints}%
{}%
{\node[titregris,left,xshift=-1cm] at (P)%
{~\mdf@frametitlefont{\mdf@@exercisepoints points}~};}%
},
}
\makeatother


%%%%%%%%%

%%%%%%%%%%%%%%%
\mdfdefinestyle{theoremstyle}{%
outerlinewidth=0.01em,linecolor=black,middlelinewidth=0.5pt,%
frametitlerule=true,roundcorner=2pt,%
apptotikzsetting={\tikzset{mfframetitlebackground/.append style={%
shade,left color=white, right color=blue!20}}},
frametitlerulecolor=black,innertopmargin=1\baselineskip,%green!60,
innerbottommargin=0.5\baselineskip,
frametitlerulewidth=0.1pt,
innertopmargin=0.7\topskip,skipabove={\dimexpr0.2\baselineskip+0.1\topskip\relax},
frametitleaboveskip=1pt,
frametitlebelowskip=1pt
}
\setlength{\parskip}{0mm}
\setlength{\parindent}{10mm}
\mdtheorem[style=theoremstyle]{exercise}{\textbf{Exercise}}

%================Liste definition--numList-and alphList=============
\newcounter{alphListCounter}
\newenvironment
{alphList}
{\begin{list}
{\alph{alphListCounter})}
{\usecounter{alphListCounter}
\setlength{\rightmargin}{0cm}
\setlength{\leftmargin}{0.5cm}
\setlength{\itemsep}{0.2cm}
\setlength{\partopsep}{0cm}
\setlength{\parsep}{0cm}}
}
{\end{list}}
\newcounter{numListCounter}
\newenvironment
{numList}
{\begin{list}
{\arabic{numListCounter})}
{\usecounter{numListCounter}
\setlength{\rightmargin}{0cm}
\setlength{\leftmargin}{0.5cm}
\setlength{\itemsep}{0cm}
\setlength{\partopsep}{0cm}
\setlength{\parsep}{0cm}}
}
{\end{list}}

\usepackage[breaklinks=true,letterpaper=true,linkcolor=magenta,urlcolor=magenta,citecolor=black]{hyperref}

\usepackage{cleveref}


%===========================================================
\begin{document}

\begin{center}
  \large{\textbf{SYDE675: Introduction to Pattern Recognition} \\ Assignment 2\\ \red{Due: 11:59 PM (EST), Feb 27, 2024}, submit on LEARN.} \\

Include your name and student number!

\end{center}

\begin{center}
Submit your write-up in pdf and all source code in a zip file (with proper documentation). Write a script for each programming exercise so that the TAs can easily run and verify your results. Make sure your code runs!

[Text in square brackets are hints that can be ignored.]
\end{center}

\noindent \red{NOTE} For all the exercises, you are only allowed to use the basic Python, Numpy, and matplotlib (for plotting) libraries, unless specified otherwise.

\begin{exercise}[Parametric and Non-Parametric Estimation (35 pts)]
In this exercise, you will be using the \href{https://pytorch.org/vision/stable/datasets.html#mnist}{\magenta{MNIST}} dataset for image classification. To work with this dataset, you will first need to flatten your images from 28$\times$28 to 784$\times$1 vectors. Next, use the \href{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}{\magenta{PCA}} in scikit learn to convert the 784$\times$1 vectors to 1$\times$1 vectors (or real numbers). \blue{Note} that the dataset consists of the training and the test sets. Use the training set for implementing the classifiers in the exercise. \blue{Also}, use only two classes in the dataset i.e. the two classes representing numbers 3 and 4.

\begin{enumerate} 
    \item (10 pts) We derived the expressions for the mean and standard deviation of the Gaussian distribution in class. Now, let's assume the probability distribution of your dataset is not just a single Gaussian distribution but a product of a Gaussian distribution and an exponential distribution:
    \begin{equation}
        p(X|\theta) = \frac{1}{2} [\mathcal{N}(X|\theta_1,\theta_2) \times f(x;\lambda)] 
    \end{equation}
    In this case, you will be estimating three parameters ($\theta_1$, $\theta_2$, and $\lambda$), where $\theta_1$ and $\theta_2$ are the mean and variance of the Gaussian distribution. Using MLE, derive an expression for finding the three parameters of your probability distribution defined by eq. (2).
    \item (5 pts) Using the formula derived for the above distribution and the parameters derived for a Gaussian distribution in class, develop two ML classifiers using the training data of the two MNIST classes. Test the two ML classifiers on the test set of the MNIST dataset. Which of the two ML classifiers is the best? Why?  
    \item (15 pts) Now let's move on to the non-parametric estimation. User kernel-based density estimation with a Gaussian kernel of $\sigma=20$ to estimate the probability distributions of the two MNIST classes. Note that $\sigma$ represents the scaling factor for the Gaussian kernel. Use an ML-based classifier using this probability estimation, and report the test accuracy on the MNIST dataset. 
    \item (5 pts) Comparing the non-parametric and the two parametric estimation methods, which one is the best? Explain.
\end{enumerate}
\end{exercise}

\begin{exercise}[K-means clustering (35 pts)]
\red{NOTE}: You are allowed to compare your results with the Scikit implementation of k-means, but you cannot use the Scikit implementation as your own solution.  

\noindent In this exercise you will be using the MNIST dataset as in the previous exercise. You will also be using 784$\times$1 directly without any PCA. You can use the training data of the MNIST dataset for this exercise. Use all the classes in the dataset. 

\begin{enumerate}
    \item (10 pts) In the class we looked at the K-means clustering algorithm with the Euclidean distance. Now, let's implement the k-means clustering algorithm with two different distance metrics, the cosine distance and the Mahalanobis distance. You can use the scikit implementation of the cosine distance. For the Mahalanobis distance, you do not need to calculate the inverse of the covariance using eigenvalue decomposition.  
    \item (5 pts) Apply your k-means implementation to the MNIST dataset. Use k=\{5,10,20,40, 200\}. Also, you will not be using class labels during the clustering process as it is an unsupervised learning problem. 
    \item (5 pts) As we do not use class labels for clustering we cannot use test error to evaluate the k-means algorithm. Instead, we can test our clustering implementation using cluster consistency: all data points in a cluster should belong to the same class. For example, if your cluster number 1 has all the data points belonging to class ``5'' then this cluster has perfect consistency. You can find the cluster consistency $Q_i$ for each cluster $i$ as follows:
    \begin{equation}
        Q_i = \frac{m_i}{N_i}
    \end{equation}
    where, $N_i$ is the total number of data points in cluster $i$, and $m_i$ represents the total number of data points belonging to the most common class in cluster $i$. You can find $m_i$ by first counting the total number of data points belonging to each of the 10 classes in cluster $i$, and then taking the max of this list. To find the overall clustering consistency:
    \begin{equation}
    Q = \frac{1}{k}\sum_{i=1}^{k} Q_i
    \end{equation}
    Report the cluster consistency of your k-means clustering on the MNIST dataset for all four values of $k$. 
   \item (5 pts) Which $k$ value produces the best results? Explain. Can the results from cluster consistency be misleading? Explain. [\blue{HINT} Intuitively, what $k$ value should produce the best results on the MNIST dataset?]
   \item (10 pts) What can you do to further validate your results if the cluster consistency metric is not working? Can you use the objective function defined in the class to find out the internal cluster distance of the data points from the mean? How can this objective help determine any misleading clustering results? Explain and demonstrate this method on the clustering results of the previous steps.  
   
\end{enumerate}
    
\end{exercise}

\begin{exercise}[Gaussian Mixture Model (GMM) (30 pts)]
    For a \href{https://en.wikipedia.org/wiki/Diagonal_matrix}{\textcolor{magenta}{diagonal matrix}} $diag(\sv)$, $|diag(\sv)| = \prod_{i} s_i$.
	
	\noindent \textcolor{blue}{Note:} For 2.1, you only need to implement (or analyze) your algorithm without testing it on any dataset. Implementations in 2.1 will be tested in 2.2. Therefore, any questions related to the testing of your implementation in 2.1 on a dataset, can be answered in 2.2.
	
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\KwIn{$X\in\RR^{n\times d}$, $K\in \NN$, initialization for $model$}
		\tcp{$model$ includes $\pi\in\RR^K_+$ and for each $1\leq k \leq K$, $\boldsymbol{\mu}_k \in \RR^d$ and $S_k\in\mathbb{S}^d_+$}
		\tcp{$\pi_k \geq 0$, $\sum_{k=1}^K \pi_k = 1$, $S_k$ symmetric and positive definite.}
		\tcp{random initialization suffices for full credit.}
		\tcp{alternatively, can initialize $r$ by randomly assigning each data to one of the $K$ components}
		\KwOut{$model, \ell$}
		
		\For{$iter = 1: \textsc{maxiter}$}{
			\tcp{step 2, for each $i=1,\ldots, n$}
			\For{$k=1, \ldots, K$}{
				\red{$r_{ik} \gets \pi_k |S_k|^{-1/2} \exp[-\frac{1}{2} (\xv_i - \boldsymbol{\mu}_k)^\top S_k^{-1}(\xv_i - \boldsymbol{\mu}_k)]$} \tcp*{compute responsibility}			
			}
			
			\tcp{for each $i=1,\ldots, n$}
			$r_{i.} \gets \sum_{k=1}^K r_{ik}$
			
			\tcp{for each $k=1, \ldots, K$ and $i=1,\ldots, n$}		
			$r_{ik} \gets r_{ik} / r_{i.}$ \tcp*{normalize}
			
			\tcp{compute negative log-likelihood}
			$\ell(iter) = -\sum_{i=1}^n \log(r_{i.})$
			
			\If{$iter > 1 ~\&\&~ |\ell(iter)-\ell(iter-1)| \leq \textsc{tol}*|\ell(iter)|$}{
				\textbf{break}
			}
			
			\tcp{step 1, for each $k=1, \ldots, K$}
			$r_{.k} \gets \sum_{i=1}^n r_{ik}$
			
			$\pi_k \gets r_{.k} / n$
			
			$\boldsymbol{\mu}_k = \sum_{i=1}^n r_{ik} \xv_i / r_{.k}$
			
			\red{$S_k \gets \left(\sum_{i=1}^n r_{ik} \xv_i \xv_i^\top / r_{.k}\right) - \boldsymbol{\mu}_k \boldsymbol{\mu}_k^\top$}
		}
		\caption{EM for GMM.}
		\label{alg:gmm}
	\end{algorithm}
	
	\begin{enumerate}
		\item (15 pts) \underline{Derive and implement} the EM algorithm for the \textcolor{blue}{diagonal} Gaussian mixture model (dGMM), \textcolor{red}{where all covariance matrices are constrained to be diagonal}. Note that the above algorithm does not make any assumptions about the covariance matrices, however, in our class, we assumed the covariances to be identity matrices. \Cref{alg:gmm} recaps all the essential steps and serves as a hint rather than a verbatim instruction. In particular, you must change the highlighted steps accordingly (with each $S_k$ being a diagonal matrix), along \underline{with formal explanations.}. 
		
		[You might want to review the steps we took in class to get the updates in \Cref{alg:gmm} and adapt them to a bit more complex case here. The solution should look like $s_j = \frac{\sum_{i=1}^n r_{ik} (x_{ij} - \mu_j)^2}{\sum_{i=1}^n r_{ik} } = \frac{\sum_{i=1}^n r_{ik} x_{ij} ^2}{\sum_{i=1}^n r_{ik} } - \mu_j^2$ for the $j$-th diagonal. Multiplying an $n\times p$ matrix with a $p\times m$ matrix costs $O(mnp)$. Do not maintain a diagonal matrix explicitly; using a vector for its diagonal suffices.]
		
		To stop the algorithm, set a maximum number of iterations (say $\textsc{maxiter} = 500$) and also monitor the change of the negative log-likelihood $\ell$: 
		\begin{align}
		\ell = -\sum_{i=1}^n \log\left[\sum_{k=1}^K \pi_k  (2\pi)^{-d/2} |S_k|^{-1/2} \exp[ -\tfrac{1}{2}(\xv_i -\boldsymbol{\mu}_k)^\top S_k^{-1} (\xv_i - \boldsymbol{\mu}_k ) ]\right],
		\end{align}
		where $\xv_i$ is the $i$-th column of $X^\top$.
		\blue{As a debug tool, note that $\ell$ should decrease from step to step}, and we can stop the algorithm if the decrease is smaller than a predefined threshold, say $\textsc{tol} = 10^{-5}$.
		
		\item (15 pts) Next, we apply (the adapted) \Cref{alg:gmm} in Ex 2.1 to the \href{http://yann.lecun.com/exdb/mnist/}{\magenta{MNIST}} dataset (that you already experimented on before). For each of the 10 classes (digits), we can use its (and only its) training images to estimate its (class-conditional) distribution by fitting a GMM (with say $K=5$, roughly corresponding to 5 styles of writing this digit). This gives us the density estimate $p(\xv | y)$ where $\xv$ is an image (of some digit) and $y$ is the class (digit). We can now classify the test set using the Bayes classifier:
		\begin{align}
		\hat y(\xv) = \arg\max_{c = 0, \ldots, 9} ~~ \underbrace{\mathrm{Pr}(Y = c) \cdot p(X = \xv | Y = c)}_{\propto ~\mathrm{Pr}(Y=c | X=\xv)},
		\end{align}
		where the probabilities $\mathrm{Pr}(Y = c)$ can be estimated using the training set, \eg, the proportion of the $c$-th class in the training set, and the \red{density} $p(X = \xv | Y = c)$ is estimated using GMM for each class $c$ separately. \underline{Report your error rates on the test set} as a function of $K$. Use $5, 10, 20, 30$ as the values for $K$. Note that if time is a concern, using $K=5$ will receive full credit. 
		
		[Optional: Reduce dimension by \href{https://en.wikipedia.org/wiki/Principal_component_analysis}{\magenta{PCA}} may boost accuracy quite a bit. Your running time should be on the order of minutes (for one  $K$), if you do not introduce extra for-loops in \Cref{alg:gmm}.]
		
		[In case you are wondering, our classification procedure above belongs to the so-called plug-in estimators (plug the estimated densities to the known optimal Bayes classifier). However, note that estimating the density $p(X=\xv | Y = c)$ is actually harder than classification. Solving a problem (\eg classification) through some intermediate harder problem (\eg density estimation) is almost always a bad idea. \textcolor{blue}{Extra: 5 pts} Do you think this will be true for harder classification problems (such as \href{https://arxiv.org/pdf/1911.00155.pdf}{\magenta{indoor scene classification}})? Explain.]
		

	\end{enumerate}	
\end{exercise}

\end{document}
              